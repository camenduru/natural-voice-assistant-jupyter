{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/natural-voice-assistant-jupyter/blob/main/test2.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "# %cd /content\n",
        "# !git clone https://github.com/yl4579/StyleTTS2.git /content/inferencebot/StyleTTS2\n",
        "# %cd StyleTTS2\n",
        "# !pip install -q munch pydub accelerate phonemizer einops einops-exts git+https://github.com/resemble-ai/monotonic_align.git nltk librosa transformers -U\n",
        "# !apt install espeak-ng aria2\n",
        "# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/audo/all_test/resolve/main/LJ001-0110.wav -d /content/inferencebot/StyleTTS2/Data/wavs -o LJ001-0110.wav\n",
        "# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/LJSpeech/resolve/main/epoch_2nd_00100.pth -d /content/inferencebot/StyleTTS2/Models/LJSpeech -o epoch_2nd_00100.pth\n",
        "# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/LJSpeech/raw/main/config.yml -d /content/inferencebot/StyleTTS2/Models/LJSpeech -o config.yml\n",
        "\n",
        "%cd /content/inferencebot/StyleTTS2\n",
        "\n",
        "from typing import Iterator\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "import random\n",
        "random.seed(0)\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import time\n",
        "import random\n",
        "import yaml\n",
        "from munch import Munch\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import librosa\n",
        "from nltk.tokenize import word_tokenize\n",
        "from models import *\n",
        "from utils import *\n",
        "from text_utils import TextCleaner\n",
        "textclenaer = TextCleaner()\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "to_mel = torchaudio.transforms.MelSpectrogram(\n",
        "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
        "mean, std = -4, 4\n",
        "\n",
        "def length_to_mask(lengths):\n",
        "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
        "    mask = torch.gt(mask+1, lengths.unsqueeze(1))\n",
        "    return mask\n",
        "\n",
        "def preprocess(wave):\n",
        "    wave_tensor = torch.from_numpy(wave).float()\n",
        "    mel_tensor = to_mel(wave_tensor)\n",
        "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
        "    return mel_tensor\n",
        "\n",
        "def compute_style(path):\n",
        "    wave, sr = librosa.load(path, sr=24000)\n",
        "    audio, index = librosa.effects.trim(wave, top_db=30)\n",
        "    if sr != 24000:\n",
        "        audio = librosa.resample(audio, sr, 24000)\n",
        "    mel_tensor = preprocess(audio).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ref_s = model.style_encoder(mel_tensor.unsqueeze(1))\n",
        "        ref_p = model.predictor_encoder(mel_tensor.unsqueeze(1))\n",
        "\n",
        "    return torch.cat([ref_s, ref_p], dim=1)\n",
        "\n",
        "import phonemizer\n",
        "global_phonemizer = phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True, with_stress=True, words_mismatch='ignore')\n",
        "\n",
        "config = yaml.safe_load(open(\"/content/inferencebot/StyleTTS2/Models/LJSpeech/config.yml\"))\n",
        "\n",
        "ASR_config = config.get('ASR_config', False)\n",
        "ASR_path = config.get('ASR_path', False)\n",
        "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
        "\n",
        "F0_path = config.get('F0_path', False)\n",
        "pitch_extractor = load_F0_models(F0_path)\n",
        "\n",
        "from Utils.PLBERT.util import load_plbert\n",
        "BERT_path = config.get('PLBERT_dir', False)\n",
        "plbert = load_plbert(BERT_path)\n",
        "\n",
        "model = build_model(recursive_munch(config['model_params']), text_aligner, pitch_extractor, plbert)\n",
        "_ = [model[key].eval() for key in model]\n",
        "_ = [model[key].to(device) for key in model]\n",
        "\n",
        "params_whole = torch.load(\"/content/inferencebot/StyleTTS2/Models/LJSpeech/epoch_2nd_00100.pth\", map_location='cpu')\n",
        "params = params_whole['net']\n",
        "\n",
        "for key in model:\n",
        "    if key in params:\n",
        "        print('%s loaded' % key)\n",
        "        try:\n",
        "            model[key].load_state_dict(params[key])\n",
        "        except:\n",
        "            from collections import OrderedDict\n",
        "            state_dict = params[key]\n",
        "            new_state_dict = OrderedDict()\n",
        "            for k, v in state_dict.items():\n",
        "                name = k[7:]\n",
        "                new_state_dict[name] = v\n",
        "            model[key].load_state_dict(new_state_dict, strict=False)\n",
        "_ = [model[key].eval() for key in model]\n",
        "\n",
        "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
        "sampler = DiffusionSampler(\n",
        "    model.diffusion.diffusion,\n",
        "    sampler=ADPM2Sampler(),\n",
        "    sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0),\n",
        "    clamp=False\n",
        ")\n",
        "\n",
        "def inference(text, noise, diffusion_steps=5, embedding_scale=1):\n",
        "    text = text.strip()\n",
        "    text = text.replace('\"', '')\n",
        "    ps = global_phonemizer.phonemize([text])\n",
        "    ps = word_tokenize(ps[0])\n",
        "    ps = ' '.join(ps)\n",
        "\n",
        "    tokens = textclenaer(ps)\n",
        "    tokens.insert(0, 0)\n",
        "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(tokens.device)\n",
        "        text_mask = length_to_mask(input_lengths).to(tokens.device)\n",
        "\n",
        "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
        "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
        "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
        "\n",
        "        s_pred = sampler(noise,\n",
        "              embedding=bert_dur[0].unsqueeze(0), num_steps=diffusion_steps,\n",
        "              embedding_scale=embedding_scale).squeeze(0)\n",
        "\n",
        "        s = s_pred[:, 128:]\n",
        "        ref = s_pred[:, :128]\n",
        "\n",
        "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
        "\n",
        "        x, _ = model.predictor.lstm(d)\n",
        "        duration = model.predictor.duration_proj(x)\n",
        "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
        "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
        "\n",
        "        pred_dur[-1] += 5\n",
        "\n",
        "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
        "        c_frame = 0\n",
        "        for i in range(pred_aln_trg.size(0)):\n",
        "            pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
        "            c_frame += int(pred_dur[i].data)\n",
        "\n",
        "        en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
        "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
        "        out = model.decoder((t_en @ pred_aln_trg.unsqueeze(0).to(device)),\n",
        "                                F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
        "\n",
        "    return out.squeeze().cpu().numpy()\n",
        "\n",
        "def LFinference(text, s_prev, noise, alpha=0.7, diffusion_steps=5, embedding_scale=1):\n",
        "  text = text.strip()\n",
        "  text = text.replace('\"', '')\n",
        "  ps = global_phonemizer.phonemize([text])\n",
        "  ps = word_tokenize(ps[0])\n",
        "  ps = ' '.join(ps)\n",
        "\n",
        "  tokens = textclenaer(ps)\n",
        "  tokens.insert(0, 0)\n",
        "  tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      input_lengths = torch.LongTensor([tokens.shape[-1]]).to(tokens.device)\n",
        "      text_mask = length_to_mask(input_lengths).to(tokens.device)\n",
        "\n",
        "      t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
        "      bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
        "      d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
        "\n",
        "      s_pred = sampler(noise,\n",
        "            embedding=bert_dur[0].unsqueeze(0), num_steps=diffusion_steps,\n",
        "            embedding_scale=embedding_scale).squeeze(0)\n",
        "\n",
        "      if s_prev is not None:\n",
        "          s_pred = alpha * s_prev + (1 - alpha) * s_pred\n",
        "\n",
        "      s = s_pred[:, 128:]\n",
        "      ref = s_pred[:, :128]\n",
        "\n",
        "      d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
        "\n",
        "      x, _ = model.predictor.lstm(d)\n",
        "      duration = model.predictor.duration_proj(x)\n",
        "      duration = torch.sigmoid(duration).sum(axis=-1)\n",
        "      pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
        "\n",
        "      pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
        "      c_frame = 0\n",
        "      for i in range(pred_aln_trg.size(0)):\n",
        "          pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
        "          c_frame += int(pred_dur[i].data)\n",
        "\n",
        "      en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
        "      F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
        "      out = model.decoder((t_en @ pred_aln_trg.unsqueeze(0).to(device)),\n",
        "                              F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
        "\n",
        "  return out.squeeze().cpu().numpy(), s_pred\n",
        "\n",
        "\n",
        "import re\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "def generate(prompt, audio):\n",
        "  prompt = prompt + 'give me a short answer'\n",
        "  messages = [{\"role\": \"user\", \"content\": prompt},]\n",
        "  encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "  model_inputs = encodeds.to(llm_model.device)\n",
        "  generated_ids = llm_model.generate(model_inputs, max_new_tokens=1024, do_sample=True)\n",
        "  decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "  text = re.sub(r'\\[INST\\].*?\\[/INST\\]', '', decoded[0])\n",
        "\n",
        "  ref_s = compute_style(audio)\n",
        "  wav = inference(text, ref_s)\n",
        "  write('/content/inferencebot/out.wav', 24000, np.array(wav.data))\n",
        "  return text, '/content/inferencebot/out.wav'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!curl -L https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp -o /content/yt-dlp\n",
        "!chmod a+rx /content/yt-dlp\n",
        "\n",
        "!/content/yt-dlp \"https://www.youtube.com/watch?v=B35E8QleVhg\" -o /content/B35E8QleVhg.webm\n",
        "\n",
        "!ffmpeg -ss 0 -t 7.7 -i /content/B35E8QleVhg.webm -y -vn -acodec copy /content/audio-0-7.7-B35E8QleVhg.webm\n",
        "!ffmpeg -i /content/audio-0-7.7-B35E8QleVhg.webm output.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text, wav = generate('who are you?', '/content/output.wav')\n",
        "from IPython.display import Audio\n",
        "Audio(wav)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
